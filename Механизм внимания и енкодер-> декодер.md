
Механизмы внимания и чем их едят. 

Зададимся вопросом, что такое внимание? Обычно можно найти определение, что внимание - это сосредоточенность человека на определенной сущности, под сущностью можно понимать что угодно, действие, объект, мышление и тд. Сама по себе сущность внимания очень обсуждаемая, есть очень много видов внимания, от чего оно зависит и тд, однако из многочисленных видов внимания(произвольное непроизвольное и тд), мы рассмотрим пример механизма внимания направленную на одного человека, так же далее будут приведены аналогии с концентрацией, как одно из мер внимания, и математическим представлениями внимания в этом сфере машинного обучения. 

Начнем с главной идеи механизма внимания в машинном обучении. Например у нас есть текст: 'Надо сдавать лабораторные до срока выполнения". Возьмем второе слово "сдавать", чтобы показать близость между словами учитывая контекст, мы можем токенизировать слова данного предложения, и каждому слову присвоить вектор, например в данном случае используем можно использовать тип кодировки one-hot, так как объем словаря маленький, далее мы считаем скалярное произведение с другими словами(включая себя же), получается 6-мерный вектор, состоящий из весов, вес - обозначает значимость слов в контексте, для удобства нормализуем эти вектора с использованием функции softmax. 
![[Pasted image 20231114210541.png]]
Таким образом, у нас получаются нормализированные вектора весов, которые пригодятся в следующей работе.

Далее рассмотрим простейшие структуры енкодера и декодера:
	Енкодер - это рекуррентная сеть, которая принимает на вход текст, и после работы в финальной ячейке хранит вектор скрытого состояния, который накопил в себе обо всем source-предложении (context vector). Этот вектор поступает как первый вектор скрытого состояние в декодер.
	Декодер - тоже реккуретная сеть, у который первый вектор скрытого состояние - вектор из енкодера. Затем на вход первой ячейки декодера подаётся служебный токен **<BOS> (begin of sentence). На выходе этой ячейки сеть обучают выдавать слово-перевод "кот" (см. рисунок). Для этого выходы ячеек пропускают через линейный слой (fc) с числом нейронов равным числу слов в словаре. Затем softmax-функция (sm), выдаёт "вероятности" слов из которых выбирается номер максимальной (argmax). Вектор полученного слова "кот" передаётся на вход второй ячейки и т.д. пока не получится служебный токен <ЕOS> (end of sentence).
	
	Valpha - вектор скрытого состояние на ячейке альфа в енкодере, u - в декодере, следующий вектор скрытого состояние считается как его сумма со взвешенным скрытым состоянием энкодера
![[Pasted image 20231114214242.png]]
Веса внимания к скрытым состояниям декодера определяются следующим образом
![[Pasted image 20231114215150.png]]
Функция внимания.
Пусть есть три матрицы: матрица Q **запросов** (query), матрица K **ключей** (key) и матрица V **значений** (value). Введенные матрицы являются аргументами **функции внимания**
![[Pasted image 20231115101350.png]]
Вернёмся теперь к механизму внимания, описанному в предыдущем разделе. Пусть U=uαi - векторы скрытых состояний декодера, а V=vαi - энкодера, где первый индекс - номер вектора, а второй - его компоненты (каждая строчка матриц U и V это α-тый вектор). Тогда, компоненты векторов u′α - добавок к скрытому состоянию декодера являются строками матрицы (μ=1/√E):
![[Pasted image 20231115102718.png]]
**Объяснение почему корень из размерности:принято делить на корень из размерности E векторов матриц Q и K. Мотивация для этого может быть следующей. Пусть компоненты двух векторов q и k являются независимыми случайными величинами с нулевым средним и единичной дисперсией. Тогда скалярное произведение q⋅k также имеет нулевое среднее и дисперсию равную размерности векторов E, т.е. типичные значения q⋅k находятся в интервале ±√E. Масштабирование переводит их к интервалу ±1**
![[Pasted image 20231115103410.png]]
Фокусы внимания - головы. H - количество голов внимания В исходной статье ([2017](https://arxiv.org/abs/1706.03762)), где были введены головы внимания, было положено Ei=Eh=E/H
По факту входящие матрицы Q,K,V сначала подвергаются линейному преобразованию, а _затем_ разрезаются по вертикали на H голов и веса внимания вычисляются уже независимым образом для каждой головы.
![[Pasted image 20231115104059.png]]

Рассмотрим рекуррентное  внимание для длинных текстов на примере RAN(Reccurent attention network) ([Краткий рассказ](https://arxiv.org/pdf/2306.06843.pdf)) 
![[Pasted image 20231114155317.png]]. 

Не будем глубоко углубляться в математику, если захотите прочесть эту статью, оставлю на неё ссылочку, статья была написана в июле. Самая идея заключается в том, что в рекуррентной сети внимания в отличие от обычных и популярных рекуррентных сетей внимания таких как LSTM и GRU, которые были созданы для избавления от проблемы затухания градиента в ванильной РНН, так как проблема затухания градиента не позволяла запоминать информацию на длительной дистанции, обычный слой заменен на слой внимания, который работает очень интересно. 
Ran проходит через последовательности без повторения на окна, она обращается к pMHSA(позиционные множественным головам внимания ) в оконной области для выделения локальных зависимостей. Для дальнейшего распространения RAN создает GPC(Global perception cell) vector из репрезентации слоя самовнимания текущего окна. GPC вектор далее конкатенируется с токенами следущего окна в который входит слой само-внимания. Новый GPC вектор будет идти к последующим окнам с остаточным смыслом, для уменьшения градиентного затухания и для обновления в той же манере. 
Сама по себе функция GPC вектора двойственна:
1)Представление уровненой контекстной информации
2)Он сохраняет очень хорошо



	Функцию внимания можно описать как сопоставление запроса (query) и набора пар ключ-значение (key-value) с выходными данными, где запрос, ключи, значения и выходные данные являются векторами. Результат вычисляется как взвешенная сумма значений, где вес, присвоенный каждому значению, вычисляется функцией совместимости запроса и соответствующего ключа.
